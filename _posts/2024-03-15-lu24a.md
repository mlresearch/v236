---
title: Causal State Distillation for Explainable Reinforcement Learning
booktitle: Proceedings of the Third Conference on Causal Learning and Reasoning
year: '2024'
volume: '236'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
openreview: eEbvt9qtIR
abstract: 'Reinforcement learning (RL) is a powerful technique for training intelligent
  agents, but understanding why these agents make specific decisions can be quite
  challenging. This lack of transparency in RL models has been a long-standing problem,
  making it difficult for users to grasp the reasons behind an agent’s behaviour.
  Various approaches have been explored to address this problem, with one promising
  avenue being reward decomposition (RD). RD is appealing as it sidesteps some of
  the concerns associated with other methods that attempt to rationalize an agent’s
  behaviour in a post-hoc manner. RD works by exposing various facets of the rewards
  that contribute to the agent’s objectives during training. However, RD alone has
  limitations as it primarily offers insights based on sub-rewards and does not delve
  into the intricate cause-and-effect relationships that occur within an RL agent’s
  neural model. In this paper, we present an extension of RD that goes beyond sub-rewards
  to provide more informative explanations. Our approach is centred on a causal learning
  framework that leverages information-theoretic measures for explanation objectives
  that encourage three crucial properties of causal factors: <em>causal sufficiency,
  sparseness,</em> and <em>orthogonality</em>. These properties help us distill the
  cause-and-effect relationships between the agent’s states and actions or rewards,
  allowing for a deeper understanding of its decision-making processes. Our framework
  is designed to generate local explanations and can be applied to a wide range of
  RL tasks with multiple reward channels. Through a series of experiments, we demonstrate
  that our approach offers more meaningful and insightful explanations for the agent’s
  action selections.'
layout: inproceedings
issn: 2640-3498
id: lu24a
tex_title: Causal State Distillation for Explainable Reinforcement Learning
firstpage: 106
lastpage: 142
page: 106-142
order: 106
cycles: false
bibtex_editor: Locatello, Francesco and Didelez, Vanessa
editor:
- given: Francesco
  family: Locatello
- given: Vanessa
  family: Didelez
bibtex_author: Lu, Wenhao and Zhao, Xufeng and Fryen, Thilo and Lee, Jae Hee and Li,
  Mengdi and Magg, Sven and Wermter, Stefan
author:
- given: Wenhao
  family: Lu
- given: Xufeng
  family: Zhao
- given: Thilo
  family: Fryen
- given: Jae Hee
  family: Lee
- given: Mengdi
  family: Li
- given: Sven
  family: Magg
- given: Stefan
  family: Wermter
date: 2024-03-15
address:
container-title: Proceedings of the Third Conference on Causal Learning and Reasoning
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v236/lu24a/lu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
