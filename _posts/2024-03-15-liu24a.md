---
title: Implicit and Explicit Policy Constraints for Offline Reinforcement Learning
booktitle: Proceedings of the Third Conference on Causal Learning and Reasoning
year: '2024'
volume: '236'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
openreview: hIp9PcLUEW
abstract: Offline reinforcement learning (RL) aims to improve the target policy over
  the behavior policy based on historical data. A major problem of offline RL is the
  distribution shift that causes overestimation of the Q-value due to out-of-distribution
  actions. Most existing works focus on either behavioral cloning (BC) or maximizing
  Q-Learning methods to suppress distribution shift. BC methods try to mitigate the
  shift by constraining the target policy to be close to the offline data, but it
  makes the learned policy highly conservative. On the other hand, maximizing Q-Learning
  methods adopt pessimism mechanism to generate actions by maximizing Q-value and
  penalizing Q-value according to the uncertainty of actions. However, the generated
  actions might be arbitrary, resulting in the predicted Q-values highly uncertain,
  which will in turn misguide the policy to generate the next action. To alleviate
  the adverse effect of the distribution shift, we propose to constrain the policy
  implicitly and explicitly by unifying Q-Learning and behavior cloning to tackle
  the exploration and exploitation dilemma.  For the implicit constraint approach,
  we propose to unify the action space by generative adversarial networks that dedicate
  to make the actions of the target policy and behavior policy indistinguishable.
  For the explicit constraint approach, we propose multiple importance sampling (MIS)
  to learn an advantage weight for each state-action pair which is then used to suppress
  or make full use of each state-action pair. Extensive experiments on the D4RL dataset
  indicate that our approaches can achieve superior performance. The results on the
  Maze2D data indicate that MIS addresses heterogeneous data better than single importance
  sampling. We also found that MIS can stabilize the reward curve effectively.
layout: inproceedings
issn: 2640-3498
id: liu24a
tex_title: Implicit and Explicit Policy Constraints for Offline Reinforcement Learning
firstpage: 499
lastpage: 513
page: 499-513
order: 499
cycles: false
bibtex_editor: Locatello, Francesco and Didelez, Vanessa
editor:
- given: Francesco
  family: Locatello
- given: Vanessa
  family: Didelez
bibtex_author: Liu, Yang and Hofert, Marius
author:
- given: Yang
  family: Liu
- given: Marius
  family: Hofert
date: 2024-03-15
address:
container-title: Proceedings of the Third Conference on Causal Learning and Reasoning
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v236/liu24a/liu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
