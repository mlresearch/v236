---
title: 'Designing monitoring strategies for deployed machine learning algorithms:
  navigating performativity through a causal lens'
booktitle: Proceedings of the Third Conference on Causal Learning and Reasoning
year: '2024'
volume: '236'
series: Proceedings of Machine Learning Research
month: 0
publisher: PMLR
openreview: 0kp0JcbHXL
abstract: After a machine learning (ML)-based system is deployed, monitoring its performance
  is important to ensure the safety and effectiveness of the algorithm over time.
  When an ML algorithm interacts with its environment, the algorithm can affect the
  data-generating mechanism and be a major source of bias when evaluating its standalone
  performance, an issue known as performativity. Although prior work has shown how
  to <em>validate</em> models in the presence of performativity using causal inference
  techniques, there has been little work on how to <em>monitor</em> models in the
  presence of performativity. Unlike the setting of model validation, there is much
  less agreement on which performance metrics to monitor. Different monitoring criteria
  impact how interpretable the resulting test statistic is, what assumptions are needed
  for identifiability, and the speed of detection. When this choice is further coupled
  with the decision to use observational versus interventional data, ML deployment
  teams are faced with a multitude of monitoring options. The aim of this work is
  to highlight the relatively under-appreciated complexity of designing a monitoring
  strategy and how causal reasoning can provide a systematic framework for choosing
  between these options. As a motivating example, we consider an ML-based risk prediction
  algorithm for predicting unplanned readmissions. Bringing together tools from causal
  inference and statistical process control, we consider six monitoring procedures
  (three candidate monitoring criteria and two data sources) and investigate their
  operating characteristics in simulation studies. Results from this case study emphasize
  the seemingly simple (and obvious) fact that <em>not all monitoring systems are
  created equal</em>, which has real-world impacts on the design and documentation
  of ML monitoring systems.
layout: inproceedings
issn: 2640-3498
id: feng24a
tex_title: 'Designing monitoring strategies for deployed machine learning algorithms:
  navigating performativity through a causal lens'
firstpage: 587
lastpage: 608
page: 587-608
order: 587
cycles: false
bibtex_editor: Locatello, Francesco and Didelez, Vanessa
editor:
- given: Francesco
  family: Locatello
- given: Vanessa
  family: Didelez
bibtex_author: Feng, Jean and Subbaswamy, Adarsh and Gossmann, Alexej and Singh, Harvineet
  and Sahiner, Berkman and Kim, Mi-Ok and Pennello, Gene Anthony and Petrick, Nicholas
  and Pirracchio, Romain and Xia, Fan
author:
- given: Jean
  family: Feng
- given: Adarsh
  family: Subbaswamy
- given: Alexej
  family: Gossmann
- given: Harvineet
  family: Singh
- given: Berkman
  family: Sahiner
- given: Mi-Ok
  family: Kim
- given: Gene Anthony
  family: Pennello
- given: Nicholas
  family: Petrick
- given: Romain
  family: Pirracchio
- given: Fan
  family: Xia
date: 2024-03-15
address:
container-title: Proceedings of the Third Conference on Causal Learning and Reasoning
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v236/feng24a/feng24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
