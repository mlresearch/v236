@Proceedings{CLeaR2024,
	title = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	name = {Causal Learning and Reasoning},
	shortname = {CLeaR},
	year = {2024},
	start = {2024-04-01},
	end = {2024-04-03},
	published = {2024-03-15},
	address = {Los Angeles, California, USA},
	conference_url = {https://www.cclear.cc/2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	volume = {236}
}

@InProceedings{pmlr-v236-strieder24a,
	title = {Dual Likelihood for Causal Inference under Structure Uncertainty},
	author = {Strieder, David and Drton, Mathias},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1--17},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {cS8vwvM4rV},
	abstract = {Knowledge of the underlying causal relations is essential for inferring the effect of interventions in complex systems. In a widely studied approach, structural causal models postulate noisy functional relations among interacting variables, where the underlying causal structure is then naturally represented by a directed graph whose edges indicate direct causal dependencies. In the typical application, this underlying causal structure must be learned from data, and thus, the remaining structure uncertainty needs to be incorporated into causal inference in order to draw reliable conclusions. In recent work, test inversions provide an ansatz to account for this data-driven model choice and, therefore, combine structure learning with causal inference. In this article, we propose the use of dual likelihood to greatly simplify the treatment of the involved testing problem. Indeed, dual likelihood leads to a closed-form solution for constructing confidence regions for total causal effects that rigorously capture both sources of uncertainty: causal structure and numerical size of nonzero effects. The proposed confidence regions can be computed with a bottom-up procedure starting from sink nodes. To render the causal structure identifiable, we develop our ideas in the context of linear causal relations with equal error variances.}
}

@InProceedings{pmlr-v236-marmarelis24a,
	title = {Ensembled Prediction Intervals for Causal Outcomes Under Hidden Confounding},
	author = {Marmarelis, Myrl G and Steeg, Greg Ver and Galstyan, Aram and Morstatter, Fred},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {18--40},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {UGz4FnbEC0},
	abstract = {Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Recent work has extended prediction intervals with finite-sample guarantees to partially identifiable causal outcomes, by means of a sensitivity model for hidden confounding. In deep learning, predictors can exploit their inductive biases for better generalization out of sample. We argue that the structure inherent to a deep ensemble should inform a tighter partial identification of the causal outcomes that they predict. We therefore introduce an approach termed <b>Caus-Modens</b>, for characterizing <b>caus</b>al outcome intervals by <b>mod</b>ulated <b>ens</b>embles. We present a simple approach to partial identification using existing causal sensitivity models and show empirically that Caus-Modens gives tighter outcome intervals, as measured by the necessary interval size to achieve sufficient coverage. The last of our three diverse benchmarks is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.}
}

@InProceedings{pmlr-v236-rajendran24a,
	title = {An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis},
	author = {Rajendran, Goutham and Reizinger, Patrik and Brendel, Wieland and Ravikumar, Pradeep Kumar},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {41--70},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {EJxavhLXZz},
	abstract = {We investigate the relationship between system identification and intervention design in dynamical systems. While previous research demonstrated how identifiable representation learning methods, such as Independent Component Analysis (ICA), can reveal cause-effect relationships, it relied on a passive perspective without considering how to collect data. Our work shows that in Gaussian Linear Time-Invariant (LTI) systems, the system parameters can be identified by introducing diverse intervention signals in a multi-environment setting. By harnessing appropriate diversity assumptions motivated by the ICA literature, our findings connect experiment design and representational identifiability in dynamical systems. We corroborate our findings on synthetic and (simulated) physical data. Additionally, we show that Hidden Markov Models, in general, and (Gaussian) LTI systems, in particular,  fulfil a generalization of the Causal de Finetti theorem with continuous parameters. The project's repository is at [github.com/rpatrik96/lti-ica](https://github.com/rpatrik96/lti-ica).}
}

@InProceedings{pmlr-v236-ng24a,
	title = {Structure Learning with Continuous Optimization: A Sober Look and Beyond},
	author = {Ng, Ignavier and Huang, Biwei and Zhang, Kun},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {71--105},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {vmTxPJxD26},
	abstract = {This paper investigates in which cases continuous optimization for directed acyclic graph (DAG) structure learning can and cannot perform well and why this happens, and suggests possible directions to make the search procedure more reliable. Reisach et al. (2021) suggested that the remarkable performance of several continuous structure learning approaches is primarily driven by a high agreement between the order of increasing marginal variances and the topological order, and demonstrated that these approaches do not perform well after data standardization. We analyze this phenomenon for continuous approaches assuming equal and non-equal noise variances, and show that the statement may not hold in either case by providing counterexamples, justifications, and possible alternative explanations. We further demonstrate that nonconvexity may be a main concern especially for the non-equal noise variances formulation, while recent advances in continuous structure learning fail to achieve improvement in this case. Our findings suggest that future works should take into account the non-equal noise variances formulation to handle more general settings and for a more comprehensive empirical evaluation. Lastly, we provide insights into other aspects of the search procedure, including thresholding and sparsity, and show that they play an important role in the final solutions.}
}

@InProceedings{pmlr-v236-lu24a,
	title = {Causal State Distillation for Explainable Reinforcement Learning},
	author = {Lu, Wenhao and Zhao, Xufeng and Fryen, Thilo and Lee, Jae Hee and Li, Mengdi and Magg, Sven and Wermter, Stefan},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {106--142},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {eEbvt9qtIR},
	abstract = {Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an extension of RD that goes beyond sub-rewards to provide more informative explanations. Our approach is centred on a causal learning framework that leverages information-theoretic measures for explanation objectives that encourage three crucial properties of causal factors: <em>causal sufficiency, sparseness,</em> and <em>orthogonality</em>. These properties help us distill the cause-and-effect relationships between the agent's states and actions or rewards, allowing for a deeper understanding of its decision-making processes. Our framework is designed to generate local explanations and can be applied to a wide range of RL tasks with multiple reward channels. Through a series of experiments, we demonstrate that our approach offers more meaningful and insightful explanations for the agent's action selections.}
}

@InProceedings{pmlr-v236-curth24a,
	title = {Cautionary Tales on Synthetic Controls in Survival Analyses},
	author = {Curth, Alicia and Poon, Hoifung and Nori, Aditya V. and Gonz\'alez, Javier},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {143--159},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {fqgWuu9vJo},
	abstract = {Synthetic control (SC) methods have gained rapid popularity in economics recently, where they have been applied in the context of inferring the effects of treatments on standard continuous outcomes assuming linear input-output relations. In medical applications, conversely, survival outcomes are often of primary interest, a setup in which both commonly assumed data-generating processes (DGPs) and target parameters are different. In this paper, we therefore investigate whether and when SCs could serve as an alternative to matching methods in survival analyses. We find that, because SCs rely on a linearity assumption, they will generally be biased for the true expected survival time in commonly assumed survival DGPs -- even when taking into account the possibility of linearity on another scale as in accelerated failure time models. Additionally, we find that, because SC units follow distributions with lower variance than real control units, summaries of their distributions, such as survival curves,  will be biased for the parameters of interest in many survival analyses. Nonetheless, we also highlight that using SCs can still improve upon matching whenever the biases described above are outweighed by extrapolation biases exhibited by imperfect matches, and investigate the use of regularization to trade off the shortcomings of both approaches.}
}

@InProceedings{pmlr-v236-geiger24a,
	title = {Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations},
	author = {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {160--187},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {zHHKbpDude},
	abstract = {Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases—distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to uncovering conceptual structure in trained neural nets.}
}

@InProceedings{pmlr-v236-simoes24a,
	title = {Fundamental Properties of Causal Entropy and Information Gain},
	author = {Simoes, Francisco N. F. Q. and Dastani, Mehdi and Ommen, Thijs van},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {188--208},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {RmvWdgI37D},
	abstract = {Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when  intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality.}
}

@InProceedings{pmlr-v236-rohbeck24a,
	title = {Bicycle: Intervention-Based Causal Discovery with Cycles},
	author = {Rohbeck, Martin and Clarke, Brian and Mikulik, Katharina and Pettet, Alexandra and Stegle, Oliver and Ueltzh\"offer, Kai},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {209--242},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {vXLYseAcv4},
	abstract = {While a growing number of algorithms for causal discovery of directed acyclic graphs from observational and interventional data have been proposed, the robust identification of cyclic causal graphs in particular remains an open problem. Solutions to this challenge would have a considerable impact in various application domains, including single-cell genomics, where gene regulatory networks are known to contain feedback loops. Recent work has shown promise to address this challenge by describing the expression states in a population of cells as the steady-state solution of a stochastic dynamical system. However, this formulation cannot account for information on interventions in the population, and consequently, it ignores the associated causal inductive biases, which are key assets to obtain meaningful results and improve identifiability. In this work, we propose a novel method, Bicycle, which (i) infers cyclic causal relationships from i.i.d. data, (ii) explicitly accounts for information on the perturbation state of cells by a realization of the independent causal mechanism principle and (iii) models causal effects in a latent space rather than on observed data. We benchmark Bicycle in the context of existing approaches, demonstrating improved recovery of simulated causal graphs and improved out-of-distribution prediction performance on unseen perturbations in real single-cell datasets.}
}

@InProceedings{pmlr-v236-gultchin24a,
	title = {Pragmatic Fairness: Developing Policies with Outcome Disparity Control},
	author = {Gultchin, Limor and Guo, Siyuan and Malek, Alan and Chiappa, Silvia and Silva, Ricardo},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {243--264},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {t6IcULhLKM},
	abstract = {We introduce a causal framework for designing optimal policies that satisfy classes of fairness constraints. We take a pragmatic approach asking what we can do with an action space available from historical data, with no further experimentation and novel actions immediately available. We propose two different fairness constraints: a "moderation breaking" constraint which aims at reducing disparity in outcome levels across sensitive attributes to the extent the provided action space permits; and an "equal benefit" constraint which aims at distributing gain from the new and maximized policy equally across sensitive attribute levels, and thus at keeping pre-existing preferential treatment in place or avoiding the introduction of new disparity.
We introduce practical methods for implementing the constraints and illustrate their uses on experiments with semi-synthetic models.}
}

@InProceedings{pmlr-v236-dacunto24a,
	title = {Extracting the Multiscale Causal Backbone of Brain Dynamics},
	author = {D\textsc{\char13}Acunto, Gabriele and Bonchi, Francesco and Morales, Gianmarco De Francisci and Petri, Giovanni},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {265--295},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {IsELb7sAmm},
	abstract = {The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it. Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nodes related to sensory processing play a crucial role.
Finally, our analysis of individuals' multiscale causal structures confirms the existence of a causal fingerprint of brain connectivity, thus supporting from a causal perspective the existing extensive research in brain connectivity fingerprinting.}
}

@InProceedings{pmlr-v236-talon24a,
	title = {Towards the Reusability and Compositionality of Causal Representations},
	author = {Talon, Davide and Lippe, Phillip and James, Stuart and Bue, Alessio Del and Magliacane, Sara},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {296--324},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {fVC15fRDLT},
	abstract = {Causal Representation Learning (CRL) aims at identifying high-level causal factors and their relationships from high-dimensional observations, e.g., images. While most CRL works focus on learning causal representations in a single environment, in this work we instead propose a first step towards learning causal representations from temporal sequences of images that can be adapted in a new environment, or composed across multiple related environments. In particular, we introduce DECAF, a framework that detects which causal factors can be reused and which need to be adapted from previously learned causal representations. Our approach is based on the availability of intervention targets, that indicate which variables are perturbed at each time step.
Experiments on three benchmark datasets show that integrating our framework with four state-of-the-art CRL approaches leads to accurate representations in a new environment with only a few samples.}
}

@InProceedings{pmlr-v236-binkyte24a,
	title = {Causal Discovery Under Local Privacy},
	author = {Binkyte, Ruta and Pinz\'on, Carlos Antonio and Lesty\'an, Szilvia and Jung, Kangsoo and Arcolezi, H\'eber Hwang and Palamidessi, Catuscia},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {325--383},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {08jS82RqZP},
	abstract = {Differential privacy is a widely adopted framework designed to safeguard the sensitive information
of data providers within a data set. It is based on the application of controlled noise at the interface
between the server that stores and processes the data, and the data consumers. Local differential
privacy is a variant that allows data providers to apply the privatization mechanism themselves on
their data individually. Therefore, it provides protection also in contexts in which the server, or even
the data collector, cannot be trusted. The introduction of noise, however, inevitably affects the utility
of the data, particularly by distorting the correlations between individual data components. This
distortion can prove detrimental to tasks such as causal structure learning. In this paper, we consider
various well-known locally differentially private mechanisms and compare the trade-off between the
privacy they provide, and the accuracy of the causal structure produced by algorithms for causal
learning when applied to data obfuscated by these mechanisms. Our analysis yields valuable insights
for selecting appropriate local differentially private protocols for causal discovery tasks. We foresee
that our findings will aid researchers and practitioners in conducting locally private causal discovery.}
}

@InProceedings{pmlr-v236-barin-pacela24a,
	title = {On the Identifiability of Quantized Factors},
	author = {Barin-Pacela, Vit\'oria and Ahuja, Kartik and Lacoste-Julien, Simon and Vincent, Pascal},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {384--422},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {LlV35W9b5u},
	abstract = {Disentanglement  aims to recover meaningful latent ground-truth factors from the observed distribution solely, and is formalized through the theory of identifiability. The identifiability of independent latent factors is proven to be impossible in the unsupervised i.i.d. setting under a general nonlinear map from factors to observations.
In this work, however, we demonstrate that it is possible to recover <em>quantized</em> latent factors under a generic nonlinear diffeomorphism. We only assume that the latent factors have <em>independent discontinuities</em> in their density, without requiring the factors to be statistically independent. We introduce this novel form of identifiability, termed <em>quantized factor identifiability</em>, and provide a comprehensive proof of the recovery of the quantized factors.}
}

@InProceedings{pmlr-v236-jamshidi24a,
	title = {Confounded Budgeted Causal Bandits},
	author = {Jamshidi, Fateme and Etesami, Jalal and Kiyavash, Negar},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {423--461},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3Apr},
	publisher = {PMLR},
	openreview = {vl1u4RrOKw},
	abstract = {We study the problem of learning "good" interventions in a stochastic environment modeled by its underlying causal graph. Good interventions refer to interventions that maximize rewards. Specifically, we consider the setting of a pre-specified budget constraint, where interventions can have non-uniform costs. 
    We show that this problem can be formulated as maximizing the expected reward for a stochastic multi-armed bandit with side information. 
    We propose an algorithm to minimize the cumulative regret in general causal graphs. This algorithm trades off observations and interventions based on their costs to achieve the optimal reward. 
    This algorithm generalizes the state-of-the-art methods by allowing non-uniform costs and hidden confounders in the causal graph.
    Furthermore, we develop an algorithm to minimize the simple regret in the budgeted setting with non-uniform costs and also general causal graphs.
    We provide theoretical guarantees, including both upper and lower bounds, as well as empirical evaluations of our algorithms. Our empirical results showcase that our algorithms outperform the state of the art.}
}

@InProceedings{pmlr-v236-felekis24a,
	title = {Causal Optimal Transport of Abstractions},
	author = {Felekis, Yorgos and Zennaro, Fabio Massimo and Branchini, Nicola and Damoulas, Theodoros},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {462--498},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {pyKM7naJUs},
	abstract = {Causal abstraction (CA) theory establishes formal criteria for relating multiple structural causal models (SCMs) at different levels of granularity by defining maps between them. These maps have significant relevance for real-world challenges such as synthesizing causal evidence from multiple experimental environments, learning causally consistent representations at different resolutions, and linking interventions across multiple SCMs. In this work, we propose COTA, the first method to learn abstraction maps from observational and interventional data without assuming complete knowledge of the underlying SCMs. In particular, we introduce a multi-marginal Optimal Transport (OT) formulation that enforces do-calculus causal constraints, together with a cost function that relies on interventional information. We extensively evaluate COTA on synthetic and real world problems, and showcase its advantages over non-causal, independent and aggregated OT formulations. Finally, we demonstrate the efficiency of our method as a data augmentation tool by comparing it against prior art of CA learning, which assumes fully specified SCMs, on a real-world downstream task.}
}

@InProceedings{pmlr-v236-liu24a,
	title = {Implicit and Explicit Policy Constraints for Offline Reinforcement Learning},
	author = {Liu, Yang and Hofert, Marius},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {499--513},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {hIp9PcLUEW},
	abstract = {Offline reinforcement learning (RL) aims to improve the target policy over the behavior policy based on historical data. A major problem of offline RL is the distribution shift that causes overestimation of the Q-value due to out-of-distribution actions.
Most existing works focus on either behavioral cloning (BC) or maximizing Q-Learning methods to suppress distribution shift. BC methods try to mitigate the shift by constraining the target policy to be close to the offline data, but it makes the learned policy highly conservative. On the other hand, maximizing Q-Learning methods adopt pessimism mechanism to generate actions by maximizing Q-value and penalizing Q-value according to the uncertainty of actions. However, the generated actions might be arbitrary, resulting in the predicted Q-values highly uncertain, which will in turn misguide the policy to generate the next action.
To alleviate the adverse effect of the distribution shift, we propose to constrain the policy implicitly and explicitly by unifying Q-Learning and behavior cloning to tackle the exploration and exploitation dilemma. 
For the implicit constraint approach, we propose to unify the action space by generative adversarial networks that dedicate to make the actions of the target policy and behavior policy indistinguishable. For the explicit constraint approach, we propose multiple importance sampling (MIS) to learn an advantage weight for each state-action pair which is then used to suppress or make full use of each state-action pair.
Extensive experiments on the D4RL dataset indicate that our approaches can achieve superior performance. The results on the Maze2D data indicate that MIS addresses heterogeneous data better than single importance sampling. We also found that MIS can stabilize the reward curve effectively.}
}

@InProceedings{pmlr-v236-dettling24a,
	title = {On the Lasso for Graphical Continuous Lyapunov Models},
	author = {Dettling, Philipp and Drton, Mathias and Kolar, Mladen},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {514--550},
	year = {2024},
	editor = {Francesco Locatello and Vanessa Didelez},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {eJHrS4rpsa},
	abstract = {Graphical continuous Lyapunov models offer a new perspective on modeling causally interpretable dependence structure in multivariate data by treating each independent observation as a one-time cross-sectional snapshot of a temporal process. Specifically, the models assume that the observations are cross-sections of independent multivariate Ornstein-Uhlenbeck processes in equilibrium. The Gaussian equilibrium exists under a stability assumption on the drift matrix, and the equilibrium covariance matrix is determined by the continuous Lyapunov equation. Each graphical continuous Lyapunov model assumes the drift matrix to be sparse, with a support determined by a directed graph. A natural approach to model selection in this setting is to use an $\ell_1$-regularization technique that, based on a given sample covariance matrix, seeks to find a sparse approximate solution to the Lyapunov equation. We study the model selection properties of the resulting lasso technique to arrive at a consistency result.  Our detailed analysis reveals that the involved irrepresentability condition is surprisingly difficult to satisfy. While this may prevent asymptotic consistency in model selection, our numerical experiments indicate that even if the theoretical requirements for consistency are not met, the lasso approach is able to recover relevant structure of the drift matrix and is robust to aspects of model misspecification.}
}

@InProceedings{pmlr-v236-boeken24a,
	title = {Evaluating and Correcting Performative Effects of Decision Support Systems via Causal Domain Shift},
	author = {Boeken, Philip and Zoeter, Onno and Mooij, Joris},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {551--569},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {iYJgWivXX4},
	abstract = {When predicting a target variable $Y$ from features $X$, the prediction $\hat{Y}$ can be performative: an
agent might act on this prediction, affecting the value of $Y$ that we eventually observe. Performative
predictions are deliberately prevalent in algorithmic decision support, where a decision support system
(DSS) provides a prediction for an agent to affect the value of the target variable. When deploying a
DSS in high-stakes settings (e.g. healthcare, law, predictive policing, or child welfare screening) it is
imperative to carefully assess the performative effects of the DSS. In the case that the DSS serves
as an alarm for a predicted negative outcome, naive retraining of the prediction model is bound to
result in a model that underestimates the risk, due to effective workings of the previous model. In
this work, we propose to model the deployment of a DSS as causal domain shift and provide novel
cross-domain identification results for the conditional expectation $E[Y|X]$, allowing for pre- and
post-hoc assessment of the deployment of the DSS, and for re-training of a model that assesses
the risk under a baseline policy where the DSS is not deployed. Using a running example, we
empirically show that a repeated regression procedure provides a practical framework for estimating
these quantities, even when the data is affected by sample selection bias and selective labelling,
offering for a practical, unified solution for multiple forms of target variable bias.}
}

@InProceedings{pmlr-v236-pawar24a,
	title = {On the Impact of Neighbourhood Sampling to Satisfy Sufficiency and Necessity Criteria in Explainable AI},
	author = {Pawar, Urja and Beder, Christian and O\textsc{\char13}Reilly, Ruairi and O\textsc{\char13}Shea, Donna},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {570--586},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {g2wP4PZ0tr},
	abstract = {In the context of Machine Learning(ML) and Artificial Intelligence (AI), the concepts of sufficiency and necessity of features offer nuanced perspectives on the cause-and-effect relationships underlying a model's outputs. These concepts are, therefore, essential in Explainable AI (XAI) as they can provide a more holistic understanding of a ``black-box" AI model. Addressing this need, our study explored the relationships between the XAI's explanations and the sufficiency and necessity of features in data. This is achieved by emphasising the impact of neighbourhoods, which are central in generating explanations. By analysing a diverse set of neighbourhoods, we highlighted how they influence the alignment between the feature rankings by XAI and the measures of sufficiency and necessity. This work offers two contributions. First, it provides a comprehensive discussion on how XAI frameworks relate to sufficiency and necessity with respect to their operating neighbourhoods; and second, it empirically demonstrates the effectiveness of these neighbourhoods in conveying the sufficiency and necessity of features by the XAI frameworks.}
}

@InProceedings{pmlr-v236-feng24a,
	title = {Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens},
	author = {Feng, Jean and Subbaswamy, Adarsh and Gossmann, Alexej and Singh, Harvineet and Sahiner, Berkman and Kim, Mi-Ok and Pennello, Gene Anthony and Petrick, Nicholas and Pirracchio, Romain and Xia, Fan},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {587--608},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {0kp0JcbHXL},
	abstract = {After a machine learning (ML)-based system is deployed, monitoring its performance is important to ensure the safety and effectiveness of the algorithm over time.
When an ML algorithm interacts with its environment, the algorithm can affect the data-generating mechanism and be a major source of bias when evaluating its standalone performance, an issue known as performativity.
Although prior work has shown how to <em>validate</em> models in the presence of performativity using causal inference techniques, there has been little work on how to <em>monitor</em> models in the presence of performativity.
Unlike the setting of model validation, there is much less agreement on which performance metrics to monitor.
Different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection.
When this choice is further coupled with the decision to use observational versus interventional data, ML deployment teams are faced with a multitude of monitoring options.
The aim of this work is to highlight the relatively under-appreciated complexity of designing a monitoring strategy and how causal reasoning can provide a systematic framework for choosing between these options.
As a motivating example, we consider an ML-based risk prediction algorithm for predicting unplanned readmissions.
Bringing together tools from causal inference and statistical process control, we consider six monitoring procedures (three candidate monitoring criteria and two data sources) and investigate their operating characteristics in simulation studies.
Results from this case study emphasize the seemingly simple (and obvious) fact that <em>not all monitoring systems are created equal</em>, which has real-world impacts on the design and documentation of ML monitoring systems.}
}

@InProceedings{pmlr-v236-gobler24a,
	title = {$\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery},
	author = {G\"obler, Konstantin and Windisch, Tobias and Drton, Mathias and Pychynski, Tim and Roth, Martin and Sonntag, Steffen},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {609--642},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {LzsNjQDLO4},
	abstract = {Algorithms for causal discovery have recently undergone rapid advances and increasingly draw on flexible nonparametric methods to process complex data. With these advances comes a need for adequate empirical validation of the causal relationships learned by different algorithms. However, for most real and complex data sources true causal relations remain unknown. This issue is further compounded by privacy concerns surrounding the release of suitable high-quality data. To tackle these challenges, we introduce $\texttt{causalAssembly}$, a semisynthetic data generator designed to facilitate the benchmarking of causal discovery methods. The tool is built using a complex real-world dataset comprised of measurements collected along an assembly line in a manufacturing setting. For these measurements, we establish a partial set of ground truth causal relationships through a detailed study of the physics underlying the processes carried out in the assembly line. The partial ground truth is sufficiently informative to allow for estimation of a full causal graph by mere nonparametric regression. To overcome potential confounding and privacy concerns, we use distributional random forests to estimate and represent conditional distributions implied by the ground truth causal graph. These conditionals are combined into a joint distribution that strictly adheres to a causal model over the observed variables. Sampling from this distribution, $\texttt{causalAssembly}$ generates data that are guaranteed to be Markovian with respect to the ground truth. Using our tool, we showcase how to benchmark several well-known causal discovery algorithms.}
}

@InProceedings{pmlr-v236-corazza24a,
	title = {Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment},
	author = {Corazza, Jan and Aria, Hadi Partovi and Neider, Daniel and Xu, Zhe},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {643--664},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {rUaxAp2O7M},
	abstract = {Reinforcement learning (RL) algorithms struggle with learning optimal policies for tasks where reward feedback is sparse and depends on a complex sequence of events in the environment. Probabilistic reward machines (PRMs) are finite-state formalisms that can capture temporal dependencies in the reward signal, along with nondeterministic task outcomes. While special RL algorithms can exploit this finite-state structure to expedite learning, PRMs remain difficult to modify and design by hand. This hinders the already difficult tasks of utilizing high-level causal knowledge about the environment, and transferring the reward formalism into a new domain with a different causal structure. This paper proposes a novel method to incorporate causal information in the form of Temporal Logic-based Causal Diagrams into the reward formalism, thereby expediting policy learning and aiding the transfer of task specifications to new environments. Furthermore, we provide a theoretical result about convergence to optimal policy for our method, and demonstrate its strengths empirically.}
}

@InProceedings{pmlr-v236-ying24a,
	title = {Causality for Functional Longitudinal Data},
	author = {Ying, Andrew},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {665--687},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {GpUmTtYtr0},
	abstract = {``Treatment-confounder feedback'' is the central complication to resolve in longitudinal studies, to infer causality. The existing frameworks of identifying causal effects for longitudinal studies with repeated measures hinge heavily on assuming that time advances in discrete time steps or data change as a jumping process, rendering the number of ``feedbacks'' finite. However, medical studies nowadays with real-time monitoring involve functional time-varying outcomes, treatment, and confounders, which leads to an uncountably infinite number of ``feedbacks''. Therefore more general and advanced theory is needed. We generalize the definition of causal effects under user-specified stochastic treatment regimes to functional longitudinal studies with continuous monitoring and develop an identification framework for a end-of-study outcome. We provide sufficient identification assumptions including a generalized consistency assumption, a sequential randomization assumption, a positivity assumption, and a novel ``intervenable'' assumption designed for the continuous-time case. Under these assumptions, we propose a g-computation process and an inverse probability weighting process, which suggest a g-computation formula and an inverse probability weighting formula for identification. For practical purposes, we also construct two classes of population estimating equations to identify these two processes, respectively, which further suggest a doubly robust identification formula with extra robustness against process misspecification.}
}

@InProceedings{pmlr-v236-dalvi24a,
	title = {Causal Matching using Random Hyperplane Tessellations},
	author = {Dalvi, Abhishek and Ashtekar, Neil and Honavar, Vasant G},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {688--702},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {8nzE9TLMap},
	abstract = {Matching is one of the simplest approaches for estimating causal effects from observational data. Matching techniques compare the observed outcomes across pairs of individuals with similar covariate values but different treatment statuses in order to estimate causal effects. However, traditional matching techniques are unreliable given high-dimensional covariates due to the infamous curse of dimensionality. To overcome this challenge, we propose a simple, fast, yet highly effective approach to matching using Random Hyperplane Tessellations (RHPT). First, we prove that the RHPT representation is an approximate balancing score -- thus maintaining the strong ignorability assumption -- and provide empirical evidence for this claim. Second, we report results of extensive experiments showing that matching using RHPT outperforms traditional matching techniques and is competitive with state-of-the-art deep learning methods for causal effect estimation. In addition, RHPT avoids the need for computationally expensive training of deep neural networks.}
}

@InProceedings{pmlr-v236-machlanski24a,
	title = {Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice},
	author = {Machlanski, Damian and Samothrakis, Spyridon and Clarke, Paul S},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {703--739},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {hK1IEMLJrA},
	abstract = {Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains crucial to obtaining state-of-the-art performance, hyperparameter selection in ensemble settings strongly influences the choice of algorithm, in that a poor choice of hyperparameters can lead to analysts using algorithms which do not give state-of-the-art performance for their data.}
}

@InProceedings{pmlr-v236-moreira24a,
	title = {DiConStruct: Causal Concept-based Explanations through Black-Box Distillation},
	author = {Moreira, Ricardo Miguel de Oliveira and Bono, Jacopo and Cardoso, M\'ario and Saleiro, Pedro and Figueiredo, M\'ario A. T. and Bizarro, Pedro},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {740--768},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {zmu3wnJYJ0},
	abstract = {Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the predictive task performance. Despite the recent rapid advances in AI explainability, as far as we know, no method yet fulfills these three desiderata. Indeed, mainstream methods for local concept explainability do not yield causal explanations and incur a trade-off between explainability and prediction accuracy. We present DiConStruct, an explanation method that is both concept-based and causal, which produces more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Consequently, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts.}
}

@InProceedings{pmlr-v236-bacco24a,
	title = {A causality-inspired  plus-minus model for player evaluation in team sports},
	author = {Bacco, Caterina De and Wang, Yixin and Blei, David},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {769--792},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {L7uAcAzdHP},
	abstract = {We present a causality-inspired adjusted plus-minus model for evaluating individual players from their performance on a team. We take an explicitly causal approach to this problem, defining the value of a player to be the expected change in the score had we substituted the player for one who has zero value. (This quantity is “causal” in the sense that it is an inference about a hypothetical intervention.) We adapt recent ideas of factor modeling to handle the indirectly measured confounding in estimating player values, considering each player to be a “treatment” who contributes to the outcome of the game. We demonstrate the behavior of the model on data about soccer and basketball.}
}

@InProceedings{pmlr-v236-dai24a,
	title = {Inference of nonlinear causal effects with application to TWAS with GWAS summary data},
	author = {Dai, Ben and Li, Chunlin and Xue, Haoran and Pan, Wei and Shen, Xiaotong},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {793--826},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {cylRvJYxYI},
	abstract = {Large-scale genome-wide association studies (GWAS) have offered an exciting opportunity to discover putative causal genes or risk factors associated with diseases by using SNPs as instrumental variables (IVs). However, conventional approaches assume linear causal relations partly for simplicity and partly for the availability of GWAS summary data. In this work, we propose a novel model for transcriptome-wide association studies (TWAS) to incorporate nonlinear relationships across IVs, an exposure/gene, and an outcome, which is robust against violations of the valid IV assumptions, permits the use of GWAS summary data, and covers two-stage least squares (2SLS) as a special case.  We decouple the estimation of a marginal causal effect and a nonlinear transformation, where the former is estimated via sliced inverse regression and a sparse instrumental variable regression, and the latter is estimated by a ratio-adjusted inverse regression. On this ground, we propose an inferential procedure. An application of the proposed method to the ADNI gene expression data and the IGAP GWAS summary data identifies 18 causal genes associated with Alzheimer's disease, including APOE and TOMM40, in addition to 7 other genes missed by 2SLS considering only linear relationships. Our findings suggest that nonlinear modeling is required to unleash the power of IV regression for identifying potentially nonlinear gene-trait associations. The source code and accompanying software *nl-causal* can be accessed through the link: [https://github.com/statmlben/nonlinear-causal](https://github.com/statmlben/nonlinear-causal).}
}

@InProceedings{pmlr-v236-luttermann24a,
	title = {Lifted Causal Inference in Relational Domains},
	author = {Luttermann, Malte and Hartwig, Mattis and Braun, Tanya and M\"oller, Ralf and Gehrke, Marcel},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {827--842},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {pZBLuxiHmA},
	abstract = {Lifted inference exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, thereby speeding up query answering while maintaining exact answers. Even though lifting is a well-established technique for the task of probabilistic inference in relational domains, it has not yet been applied to the task of causal inference. In this paper, we show how lifting can be applied to efficiently compute causal effects in relational domains. More specifically, we introduce parametric causal factor graphs as an extension of parametric factor graphs incorporating causal knowledge and give a formal semantics of interventions therein. We further present the lifted causal inference algorithm to compute causal effects on a lifted level, thereby drastically speeding up causal inference compared to propositional inference, e.g., in causal Bayesian networks. In our empirical evaluation, we demonstrate the effectiveness of our approach.}
}

@InProceedings{pmlr-v236-bing24a,
	title = {Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions},
	author = {Bing, Simon and Ninad, Urmi and Wahl, Jonas and Runge, Jakob},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {843--867},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {7SAJ1AAdky},
	abstract = {The task of inferring high-level causal variables from low-level observations, commonly referred to as <em>causal representation learning</em>, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.}
}

@InProceedings{pmlr-v236-lopez24a,
	title = {Toward the Identifiability of Comparative Deep Generative Models},
	author = {Lopez, Romain and Huetter, Jan-Christian and Hajiramezanali, Ehsan and Pritchard, Jonathan K and Regev, Aviv},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {868--912},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {HCTK8AjpkG},
	abstract = {Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice. 
Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network). 
We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.}
}

@InProceedings{pmlr-v236-elazar24a,
	title = {Estimating the Causal Effect of Early ArXiving on Paper Acceptance},
	author = {Elazar, Yanai and Zhang, Jiayao and Wadden, David and Zhang, Bo and Smith, Noah A.},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {913--933},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {SLLC4cMz1u},
	abstract = {What is the effect of releasing a preprint of a paper before it is submitted for peer review?  
No randomized controlled trial has been conducted, so we turn to observational data to answer this question.
We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (<em>early arXiving</em>) on its acceptance to the conference.
Adjusting for confounders such as topic, authors, and quality, we may estimate the causal effect.
However, since quality is a challenging construct to estimate,
we use the <em>negative outcome control</em> method, using <em>paper citation count</em> as a control variable
to debias the quality confounding effect.
Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early arXiving does not provide an advantage to any particular group.}
}

@InProceedings{pmlr-v236-hatt24a,
	title = {Sequential Deconfounding for Causal Inference with Unobserved Confounders},
	author = {Hatt, Tobias and Feuerriegel, Stefan},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {934--956},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {E2ZUZN31Pg},
	abstract = {Observational data is often used to estimate the effect of a treatment when randomized experiments are infeasible or costly. However, observational data often yields biased estimates of treatment effects, since treatment assignment can be confounded by unobserved variables. A remedy is offered by deconfounding methods that adjust for such unobserved confounders. In this paper, we develop the Sequential Deconfounder, a method that enables estimating individualized treatment effects over time in presence of unobserved confounders. This is the first deconfounding method that can be used with a single treatment assigned at each timestep. The Sequential Deconfounder uses a novel Gaussian process latent variable model to infer substitutes for the unobserved confounders, which are then used in conjunction with an outcome model to estimate treatment effects over time. We prove that using our method yields unbiased estimates of individualized treatment responses over time. Using simulated and real medical data, we demonstrate the efficacy of our method in deconfounding the estimation of treatment responses over time.}
}

@InProceedings{pmlr-v236-hardt24a,
	title = {The PetShop Dataset — Finding Causes of Performance Issues across Microservices},
	author = {Hardt, Michaela and Orchard, William Roy and Bl\"obaum, Patrick and Kirschbaum, Elke and Kasiviswanathan, Shiva},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {957--978},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {ul1YRcETPR},
	abstract = {Identifying root causes for unexpected or undesirable behavior in complex systems is a prevalent challenge. This issue becomes especially crucial in modern cloud applications that employ numerous microservices. Although the machine learning and systems research communities have proposed various techniques to tackle this problem,
there is currently a lack of standardized datasets for quantitative benchmarking. Consequently, research groups are compelled to create their own datasets for experimentation.
This paper introduces a dataset specifically designed for evaluating root cause analyses in microservice-based applications. The dataset encompasses latency, requests, and availability metrics emitted in 5-minute intervals from a distributed application.  In addition to normal operation metrics, the dataset includes 68 injected performance issues, which increase latency and reduce availability throughout the system. We showcase how this dataset can be used to evaluate the accuracy of a variety of methods spanning different causal and non-causal characterisations of the root cause analysis problem.  We hope the new dataset, available at https://github.com/amazon-science/petshop-root-cause-analysis, enables further development of techniques in this important area.}
}

@InProceedings{pmlr-v236-debeire24a,
	title = {Bootstrap aggregation and confidence measures to improve time series causal discovery},
	author = {Debeire, Kevin and Gerhardus, Andreas and Runge, Jakob and Eyring, Veronika},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {979--1007},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {QVEFIL46ao},
	abstract = {Learning causal graphs from multivariate time series is an ubiquitous challenge in all application
domains dealing with time-dependent systems, such as in Earth sciences, biology, or engineering, to
name a few. Recent developments for this causal discovery learning task have shown considerable
skill, notably the specific time-series adaptations of the popular conditional independence-based
learning framework. However, uncertainty estimation is challenging for conditional independence-
based methods. Here, we introduce a novel bootstrap approach designed for time series causal
discovery that preserves the temporal dependencies and lag-structure. It can be combined with a
range of time series causal discovery methods and provides a measure of confidence for the links
of the time series graphs. Furthermore, next to confidence estimation, an aggregation, also called
bagging, of the bootstrapped graphs by majority voting results in bagged causal discovery methods.
In this work, we combine this approach with the state-of-the-art conditional-independence-based
algorithm PCMCI+. With extensive numerical experiments we empirically demonstrate that, in
addition to providing confidence measures for links, Bagged-PCMCI+ improves in precision and
recall as compared to its base algorithm PCMCI+, at the cost of higher computational demands.
These statistical performance improvements are especially pronounced in the more challenging
settings (short time sample size, large number of variables, high autocorrelation). Our bootstrap
approach can also be combined with other time series causal discovery algorithms and can be of
considerable use in many real-world applications.}
}

@InProceedings{pmlr-v236-du24a,
	title = {Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning},
	author = {Du, Kang and Xiang, Yu},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1008--1032},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {AK01hf9BNR},
	abstract = {We study the data-generating mechanism for reconstructive SSL to shed light on its effectiveness. With an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. The condition reveals a full-rank component that preserves the label classes of $Y$, along with a redundant component. Motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new measure, $\epsilon_s$, parameterized by the rank of factorization $s$. We incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples $n$ for downstream tasks. We design two stylized experiments to compare SSL with supervised learning (SL) under different settings to support our theoretical findings.}
}

@InProceedings{pmlr-v236-cook24a,
	title = {Semiparametric Efficient Inference in Adaptive Experiments},
	author = {Cook, Thomas and Mishler, Alan and Ramdas, Aaditya},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1033--1064},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {f6bs1ASa49},
	abstract = {We consider the problem of efficient inference of the <em>Average Treatment Effect</em> in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting the asymptotic variance. Empirical results demonstrate that our methods yield narrower confidence sequences than those previously developed in the literature while maintaining time-uniform error control.}
}

@InProceedings{pmlr-v236-bach24a,
	title = {Hyperparameter Tuning for Causal Inference with Double Machine Learning: A Simulation Study},
	author = {Bach, Philipp and Schacht, Oliver and Chernozhukov, Victor and Klaassen, Sven and Spindler, Martin},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1065--1117},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {h0ecxkungr},
	abstract = {Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks. While there is an extensive literature on tuning ML learners for prediction, there is only littleguidance available on tuning ML learners for causal machine learning. In this paper, we investigate the role of hyperparameter tuning and other practical decisions for causal estimation as based on the Double Machine Learning approach by Chernozhukov et al.~(2018). Double Machine Learning relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge. We provide empirical insights on the selection and calibration of the ML methods for the performance of causal estimation. First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning. Second, we investigate the choice of ML methods and hyperparameters, including recent AutoML frameworks, and consider the relationship between their predictive power and the estimation performance for a causal parameter of interest. Third, we investigate to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics.}
}

@InProceedings{pmlr-v236-pham24a,
	title = {Scalable Counterfactual Distribution Estimation in Multivariate Causal Models},
	author = {Pham, Thong and Shimizu, Shohei and Hino, Hideitsu and Le, Tam},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1118--1140},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {102QgJegJG},
	abstract = {We consider the problem of estimating the counterfactual joint distribution of multiple quantities of interests (e.g., outcomes) in a multivariate causal model extended from the classical difference-in-difference design. Existing methods for this task either ignore the correlation structures among dimensions of the multivariate outcome by considering univariate causal models on each dimension separately and hence produce incorrect counterfactual distributions, or poorly scale even for moderate-size datasets when directly dealing with such a multivariate causal model. We propose a method that alleviates both issues simultaneously by leveraging a robust latent one-dimensional subspace of the original high-dimension space and exploiting the efficient estimation from the univariate causal model on such space. Since the construction of the one-dimensional subspace uses information from all the dimensions, our method can capture the correlation structures and produce good estimates of the counterfactual distribution. We demonstrate the advantages of our approach over existing methods on both synthetic and real-world data.}
}

@InProceedings{pmlr-v236-ribot24a,
	title = {Causal Imputation for Counterfactual SCMs:  Bridging Graphs and Latent Factor Models},
	author = {Ribot, \'Alvaro and Squires, Chandler and Uhler, Caroline},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1141--1175},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {1MeGz2P3IM},
	abstract = {We consider the task of <em>causal imputation</em>, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system. We show that, under a linearity assumption, this setup induces a <em>latent factor model</em> over the matrix of outcomes, with an additional fixed effect term. To perform causal prediction based on this model class, we introduce simple extension to the Synthetic Interventions estimator (Agarwal et al., 2020). We evaluate several matrix completion approaches on the PRISM drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches.}
}

@InProceedings{pmlr-v236-feigenbaum24a,
	title = {Causal Layering via Conditional Entropy},
	author = {Feigenbaum, Itai and Arpit, Devansh and Heinecke, Shelby and Niebles, Juan Carlos and Yao, Weiran and Xiong, Caiming and Savarese, Silvio and Wang, Huan},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1176--1191},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {eLUA4Nbrus},
	abstract = {Causal discovery aims to recover information about an unobserved causal graph from the observable data it generates. Layerings are orderings of the variables which place causes before effects. In this paper, we provide ways to recover layerings of a graph by accessing the data via a conditional entropy oracle, when distributions are discrete. Our algorithms work by repeatedly removing sources or sinks from the graph. Under appropriate assumptions and conditioning, we can separate the sources or sinks from the remainder of the nodes by comparing their conditional entropy to the unconditional entropy of their noise. Our algorithms are provably correct and run in worst-case quadratic time. The main assumptions are faithfulness and injective noise, and either known noise entropies or weakly monotonically increasing noise entropies along directed paths. In addition, we require one of either a very mild extension of faithfulness, or strictly monotonically increasing noise entropies, or expanding noise injectivity to include an additional single argument in the structural functions.}
}

@InProceedings{pmlr-v236-zhu24a,
	title = {Meaningful Causal Aggregation and Paradoxical Confounding},
	author = {Zhu, Yuchen and Budhathoki, Kailash and K\"ubler, Jonas M. and Janzing, Dominik},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1192--1217},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {lZH3jJxvRB},
	abstract = {In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. 
Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states.
On the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution; we term this natural macro interventions. We also discuss generalizations of this observation.}
}

@InProceedings{pmlr-v236-mogensen24a,
	title = {Causal discovery in a complex industrial system: A time series benchmark},
	author = {Mogensen, S{\o}ren Wengel and Rathsman, Karin and Nilsson, Per},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1218--1236},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {9SDaSrysIW},
	abstract = {Causal discovery outputs a causal structure, represented by a graph, from observed data. For time series data, there is a variety of methods, however, it is difficult to evaluate these on real data as realistic use cases very rarely come with a known causal graph to which output can be compared. In this paper, we present a dataset from an industrial subsystem at the European Spallation Source along with its causal graph which has been constructed from expert knowledge. This provides a testbed for causal discovery from time series observations of complex systems, and we believe this can help inform the development of causal discovery methodology.}
}

@InProceedings{pmlr-v236-liu24b,
	title = {Causal Discovery with Mixed Linear and Nonlinear Additive Noise Models: A Scalable Approach},
	author = {Liu, Wenqin and Huang, Biwei and Gao, Erdun and Ke, Qiuhong and Bondell, Howard and Gong, Mingming},
	booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
	pages = {1237--1263},
	year = {2024},
	editor = {Locatello, Francesco and Didelez, Vanessa},
	volume = {236},
	series = {Proceedings of Machine Learning Research},
	month = {1--3 Apr},
	publisher = {PMLR},
	openreview = {JEJExETuxg},
	abstract = {Estimating the structure of directed acyclic graphs (DAGs) from observational data is challenging due to the super-exponential growth of the search space with the number of nodes. Previous research primarily focuses on identifying a unique DAG under specific model constraints in linear or nonlinear scenarios. However, real-world scenarios often involve causal mechanisms with a mixture of linear and nonlinear characteristics, which has received limited attention in existing literature. Due to unidentifiability, existing algorithms relying on fully identifiable conditions may produce erroneous results. Although traditional methods like the PC algorithm can be employed to uncover such graphs, they typically yield only a Markov equivalence class. This paper introduces a novel causal discovery approach that extends beyond the Markov equivalence class, aiming to uncover as many edge directions as possible when the causal graph is not fully identifiable. Our approach exploits the second derivative of the log-likelihood in observational data, harnessing scalable machine learning approaches to approximate the score function. Overall, our approach demonstrates competitive accuracy comparable to current state-of-the-art techniques while offering a significant improvement in computational speed.}
}

